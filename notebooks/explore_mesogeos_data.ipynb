{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring MesoGEOS Wildfire Dataset\n",
    "\n",
    "This notebook explores the mesogeos_cube.zarr dataset to understand its structure, dimensions, and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Zarr Dataset\n",
    "\n",
    "Zarr is a format for storing chunked, compressed N-dimensional arrays. We'll use xarray to load and explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the zarr dataset\n",
    "data_path = '../data/raw/mesogeos/mesogeos_cube.zarr'\n",
    "\n",
    "# Load the dataset using xarray\n",
    "ds = xr.open_zarr(data_path)\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset type: {type(ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the complete dataset structure\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET STRUCTURE\")\n",
    "print(\"=\" * 80)\n",
    "display(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dimensions\n",
    "print(\"\\nDimensions:\")\n",
    "print(\"-\" * 40)\n",
    "for dim, size in ds.dims.items():\n",
    "    print(f\"{dim}: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coordinates\n",
    "print(\"\\nCoordinates:\")\n",
    "print(\"-\" * 40)\n",
    "for coord in ds.coords:\n",
    "    print(f\"{coord}: {ds.coords[coord].shape} - {ds.coords[coord].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all data variables\n",
    "print(\"\\nData Variables:\")\n",
    "print(\"-\" * 40)\n",
    "for var in ds.data_vars:\n",
    "    shape = ds[var].shape\n",
    "    dtype = ds[var].dtype\n",
    "    dims = ds[var].dims\n",
    "    print(f\"{var}:\")\n",
    "    print(f\"  Shape: {shape}\")\n",
    "    print(f\"  Dimensions: {dims}\")\n",
    "    print(f\"  Data type: {dtype}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Individual Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine attributes of each variable\n",
    "print(\"Variable Attributes:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for var in ds.data_vars:\n",
    "    print(f\"\\n{var}:\")\n",
    "    print(\"-\" * 40)\n",
    "    if ds[var].attrs:\n",
    "        for attr_name, attr_value in ds[var].attrs.items():\n",
    "            print(f\"  {attr_name}: {attr_value}\")\n",
    "    else:\n",
    "        print(\"  No attributes found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics for each variable\n",
    "print(\"\\nVariable Statistics:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for var in ds.data_vars:\n",
    "    print(f\"\\n{var}:\")\n",
    "    print(\"-\" * 40)\n",
    "    try:\n",
    "        data = ds[var]\n",
    "        print(f\"  Min: {float(data.min().values):.4f}\")\n",
    "        print(f\"  Max: {float(data.max().values):.4f}\")\n",
    "        print(f\"  Mean: {float(data.mean().values):.4f}\")\n",
    "        print(f\"  Std: {float(data.std().values):.4f}\")\n",
    "        \n",
    "        # Check for NaN values\n",
    "        nan_count = np.isnan(data.values).sum()\n",
    "        total_count = data.size\n",
    "        print(f\"  NaN values: {nan_count} / {total_count} ({100*nan_count/total_count:.2f}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error computing statistics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temporal Analysis\n",
    "\n",
    "Explore the temporal dimension if it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for time dimension\n",
    "time_dims = ['time', 'Time', 'date', 'timestamp']\n",
    "time_coord = None\n",
    "\n",
    "for dim in time_dims:\n",
    "    if dim in ds.dims:\n",
    "        time_coord = dim\n",
    "        break\n",
    "\n",
    "if time_coord:\n",
    "    print(f\"Time coordinate found: {time_coord}\")\n",
    "    print(f\"Number of timesteps: {len(ds[time_coord])}\")\n",
    "    print(f\"\\nFirst timestep: {ds[time_coord].values[0]}\")\n",
    "    print(f\"Last timestep: {ds[time_coord].values[-1]}\")\n",
    "    \n",
    "    # Show sample of time values\n",
    "    print(f\"\\nSample time values (first 10):\")\n",
    "    print(ds[time_coord].values[:10])\n",
    "else:\n",
    "    print(\"No standard time dimension found in dataset\")\n",
    "    print(f\"Available dimensions: {list(ds.dims.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spatial Analysis\n",
    "\n",
    "Explore spatial dimensions (latitude, longitude, x, y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for spatial coordinates\n",
    "spatial_coords = ['lat', 'latitude', 'lon', 'longitude', 'x', 'y']\n",
    "\n",
    "print(\"Spatial Coordinates:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for coord in spatial_coords:\n",
    "    if coord in ds.coords or coord in ds.dims:\n",
    "        print(f\"\\n{coord}:\")\n",
    "        if coord in ds.coords:\n",
    "            values = ds.coords[coord].values\n",
    "        else:\n",
    "            values = ds[coord].values if coord in ds else None\n",
    "        \n",
    "        if values is not None:\n",
    "            print(f\"  Range: [{values.min():.4f}, {values.max():.4f}]\")\n",
    "            print(f\"  Shape: {values.shape}\")\n",
    "            print(f\"  Resolution: {np.diff(values).mean():.6f}\" if len(values) > 1 else \"  Single value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "Create visualizations of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a sample of the first variable\n",
    "if len(ds.data_vars) > 0:\n",
    "    first_var = list(ds.data_vars)[0]\n",
    "    print(f\"Visualizing: {first_var}\")\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: First timestep/slice\n",
    "    if len(ds[first_var].shape) >= 2:\n",
    "        # Get a 2D slice\n",
    "        if len(ds[first_var].shape) == 2:\n",
    "            data_slice = ds[first_var]\n",
    "        elif len(ds[first_var].shape) == 3:\n",
    "            data_slice = ds[first_var].isel({ds[first_var].dims[0]: 0})\n",
    "        else:\n",
    "            # More than 3 dimensions, take first index of all but last 2 dims\n",
    "            isel_dict = {dim: 0 for dim in ds[first_var].dims[:-2]}\n",
    "            data_slice = ds[first_var].isel(isel_dict)\n",
    "        \n",
    "        im = data_slice.plot(ax=axes[0], cmap='RdYlBu_r', add_colorbar=True)\n",
    "        axes[0].set_title(f'{first_var} - Spatial Distribution')\n",
    "    \n",
    "    # Plot 2: Histogram\n",
    "    axes[1].hist(ds[first_var].values.flatten(), bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[1].set_xlabel('Value')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title(f'{first_var} - Distribution')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data variables available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dataset Summary\n",
    "\n",
    "Create a comprehensive summary of the dataset for documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary dictionary\n",
    "summary = {\n",
    "    'dataset_path': data_path,\n",
    "    'dimensions': dict(ds.dims),\n",
    "    'coordinates': list(ds.coords),\n",
    "    'data_variables': list(ds.data_vars),\n",
    "    'total_size_bytes': sum(ds[var].nbytes for var in ds.data_vars),\n",
    "}\n",
    "\n",
    "print(\"\\nDATASET SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Path: {summary['dataset_path']}\")\n",
    "print(f\"\\nDimensions: {summary['dimensions']}\")\n",
    "print(f\"\\nCoordinates: {summary['coordinates']}\")\n",
    "print(f\"\\nData Variables ({len(summary['data_variables'])}): {summary['data_variables']}\")\n",
    "print(f\"\\nTotal Size: {summary['total_size_bytes'] / (1024**3):.2f} GB\")\n",
    "\n",
    "# Global attributes\n",
    "if ds.attrs:\n",
    "    print(\"\\nGlobal Attributes:\")\n",
    "    print(\"-\" * 40)\n",
    "    for attr_name, attr_value in ds.attrs.items():\n",
    "        print(f\"{attr_name}: {attr_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Summary to CSV\n",
    "\n",
    "Export key information for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with variable information\n",
    "var_info = []\n",
    "\n",
    "for var in ds.data_vars:\n",
    "    var_info.append({\n",
    "        'variable': var,\n",
    "        'dimensions': str(ds[var].dims),\n",
    "        'shape': str(ds[var].shape),\n",
    "        'dtype': str(ds[var].dtype),\n",
    "        'min': float(ds[var].min().values),\n",
    "        'max': float(ds[var].max().values),\n",
    "        'mean': float(ds[var].mean().values),\n",
    "        'std': float(ds[var].std().values),\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(var_info)\n",
    "print(\"\\nVariable Summary Table:\")\n",
    "display(df_summary)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = '../data/processed/mesogeos_summary.csv'\n",
    "df_summary.to_csv(output_path, index=False)\n",
    "print(f\"\\nSummary saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "\n",
    "Based on the exploration above:\n",
    "1. Identify relevant features for wildfire prediction (temperature, wind, humidity, vegetation, etc.)\n",
    "2. Check data quality and completeness\n",
    "3. Plan preprocessing steps (normalization, handling missing values, etc.)\n",
    "4. Design the model architecture based on spatial-temporal structure\n",
    "5. Create train/validation/test splits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
